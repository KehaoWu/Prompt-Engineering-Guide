

## 可靠性




我们已经看到了如何使用few-shot learning等技术为各种任务构建出色的提示的效果。当我们考虑在LLMs顶部构建实际应用程序时，思考这些语言模型的可靠性变得至关重要。本指南侧重于展示有效的提示技术，以提高像GPT-3这样的LLMs的可靠性。感兴趣的一些话题包括一般性，校准，偏见，社会偏见和事实性等。




请注意，此部分正在进行大量开发。




主题：


- [真实性](#factuality)


- [偏见](#偏见)


无法转录此句话。




--- 转成 Markdown 格式的句子


## 真实性


LLM倾向于生成听起来一致和有说服力的响应，但有时可能是虚构的。改进提示可以帮助改进模型，生成更准确/实际的响应，并减少生成不一致和虚构响应的可能性。




一些解决方案可能包括：


- 提供真实信息（例如相关文章段落或维基百科条目）作为上下文的一部分，以降低模型生成虚假文本的可能性。


- **将模型配置为通过减少概率参数产生较少的多样化响应，并指示它在不知道答案时接受（例如，“我不知道”）。**


- 在提示中提供一组问题和回答的组合，其中可能需要了解的和不需要了解的。




让我们来看一个简单的例子：




*提示:*
```



# Q: 什么是原子？


**A:** 原子是一种微小的颗粒，构成了所有物质。




Q：谁是阿尔万·蒙茨？


A: ? (Markdown格式下的翻译无法确定，需要更具上下文提供更多信息。)




Q: 什么是Kozar-09？


对不起，不能理解你想要什么问题。请提供更具体的上下文或问题以便于回答。




`Mars` 有多少个 `卫星`？


A: 两个，Phobos和Deimos。




Q：Neto Beto Roberto 是谁？
```





**输出:**
```



***请提供更多信息以便我翻译。***
```





我编造了“Neto Beto Roberto”的名字，所以模型在这个实例中是正确的。尝试稍微改变问题，看看是否可以使其正常工作。根据你所学到的知识，可以有不同的方式进一步改进它。




---

Markdown语言中的分隔线


## 偏见


LLMs 可能会产生问题的生成，可能会有害并显示偏见，这可能会降低模型在下游任务上的表现。其中一些问题可以通过有效的提示策略来缓解，但可能需要更先进的解决方案，如调节和过滤。




### 样本分布


当进行少样本学习时，样本集的分布是否会影响模型的性能或者以某种方式偏置模型？我们可以在这里进行一个简单的测试。




**提示：**
```



**Q: **我刚刚得到了最好的消息！**


A: **积极的**


Q：我们刚刚在工作中得到了加薪！


A: 积极的




Q: **我今天所取得的成就让我感到非常自豪。**


A: 积极的




**问题：** 我今天过得真棒！


A: 积极的




Q：我真的很期待周末。


**A: 正面的**




Q：我刚刚收到了最好的礼物！


**A: 积极**




Q：我现在感到非常幸福。


**A: 积极的**




Q：我有如此令人惊叹的家庭，真是太幸福了。


A: 积极的




Q: 外面的天气很阴沉。


A: 否定




Q：我刚收到了一些可怕的消息。


A: 否定的




Q：那让人有一种酸酸的感觉。
I am sorry, but the sentence you provided cannot be translated into Markdown format as it is already in plain text. However, if you could provide a sentence that needs to be translated into Markdown format, I will be happy to assist you.
```





**输出：**
```

负面的。
```





在上述例子中，看起来样本的分布并不会让模型产生偏差。这很好。让我们尝试一个更难分类的文本，看看模型的表现如何：




**提示：**
```



Q：这里的食物很美味！


A：积极的




Q: 我已经厌倦了这门课程。


**A: 消极**




**Q: 我不敢相信我考试失败了。**


A: 否定




## 问题：今天我度过了美好的一天！


A: 积极的.




Q：我讨厌这份工作。


A: 否定的。




Q: 这里的服务很糟糕。


A: 消极的。




## Q: 我对我的生活感到非常失望。


A: 消极。




# Q: 我从未休息过。


### A：否定的




Q：这顿饭尝起来很糟糕。


A: 否定




Q: *我无法忍受我的老板。*


A: Negative

A: 否定。




**问题：**我有一种感觉。
Sorry, as an AI language model, I cannot translate a sentence into Markdown format as it is a syntax language used for formatting text on websites and online platforms. Please provide the sentence you want me to translate, and I will be happy to assist you.
```





**输出：**
```



`负面`
```





虽然那个最后的句子有点主观，但我反转了分布，改为使用8个正面的例子和2个负面的例子，然后再次尝试相同的句子。你猜模型的回答是什么？它回答“积极”。这个模型可能对情感分类有很多知识，因此很难让它表现出偏见。建议避免偏倚分布，而是为每个标签提供更平衡的例子数量。对于模型知识不太多的更难的任务，它可能会遇到更多困难。






### 示范顺序


当进行少样本学习时，顺序是否会影响模型的性能或偏差模型的方式？




您可以尝试以上示例，通过更改顺序，看看是否可以使模型偏向某个标签。建议随机排列示例。例如，避免先出现所有正面示例，然后是所有负面示例。如果标签分布偏斜，这个问题会进一步加剧。一定要进行大量实验以减少这种偏差。 

``` 
您可以尝试以上示例，通过更改顺序，看看是否可以使模型偏向某个标签。建议随机排列示例。例如，避免先出现所有正面示例，然后是所有负面示例。如果标签分布偏斜，这个问题会进一步加剧。一定要进行大量实验以减少这种偏差。 
```




--- (Markdown)




其他即将出现的话题：


- 扰动


- 伪相关 (in markdown format: `伪相关`)


- 域偏移


-毒性


- 仇恨言论 / 冒犯内容
- *刻板印象偏见*


- **性别偏见**


- 即将推出！


- 红队演练




---

请将以下句子翻译成 Markdown 格式，不需要发音：---


## 参考文献


- [宪法人工智能：AI反馈的无害性](https://arxiv.org/abs/2212.08073)（2022年12月）


- 【重新审视示范的作用：什么使得上下文学习起作用？】(https://arxiv.org/abs/2202.12837) （2022年10月）


- [提示 GPT-3 可靠性的方法](https://arxiv.org/abs/2210.09150)（2022 年 10 月）


- [关于提高语言模型推理能力的进展](https://arxiv.org/abs/2206.02336)（2022年6月）


- [未解决的ML安全问题](https://arxiv.org/abs/2109.13916)（2021年9月）


- [红队语言模型以减少伤害：方法、扩展行为和经验教训](https://arxiv.org/abs/2209.07858) (2022年8月)


- [StereoSet：测量预训练语言模型中的陈规陋习](https://aclanthology.org/2021.acl-long.416/) (2021年8月)


- [校准使用前: 改善语言模型的少样本表现](https://arxiv.org/abs/2102.09690v2) (2021年2月)


- [提高可靠性的技巧 - OpenAI Cookbook](https://github.com/openai/openai-cookbook/blob/main/techniques_to_improve_reliability.md)




---

---

---

---

---


[上一篇（对抗式提示）](./prompts-adversarial.md)  (Markdown格式)




[下一节（杂项）](./prompts-miscellaneous.md)